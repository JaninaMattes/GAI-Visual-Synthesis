{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBr7v9DWFKPC"
      },
      "source": [
        "# Exercise Sheet 8: Generative Adversarial Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 553,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7k7EcWiPFKPE",
        "outputId": "1f9adc69-0f4a-4666-eddb-d6c0bc9201c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: einops in /Users/janinaalicamattes/miniforge3/envs/pytorch-py11/lib/python3.11/site-packages (0.8.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import torchvision\n",
        "import torchvision.utils as vutils\n",
        "from torchvision import datasets, transforms\n",
        "from torch.nn.utils import spectral_norm\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "%pip install einops\n",
        "from einops import rearrange\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 554,
      "metadata": {
        "id": "XtnTWZ_IFKPF"
      },
      "outputs": [],
      "source": [
        "# folder path\n",
        "data_path = './data'\n",
        "model_path = './model'\n",
        "result_path = './results'\n",
        "\n",
        "# random seed np/torch\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Create folder if not exist\n",
        "if not os.path.exists(data_path):\n",
        "    os.makedirs(data_path)\n",
        "if not os.path.exists(model_path):\n",
        "    os.makedirs(model_path)\n",
        "if not os.path.exists(result_path):\n",
        "    os.makedirs(result_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8HgI6l6FKPF"
      },
      "source": [
        "## Task 1 a): Derive Optimal Discriminator D*\n",
        "\n",
        "Use equation (1) as a starting point to derive the optimal discriminator D∗ in terms of data probability $p_data (x)$ and generator probability $p_G(x)$.\n",
        "Assume generator G is fixed.\n",
        "\n",
        "The objective function V(D,G) is:\n",
        "\n",
        "\\begin{align}\n",
        "V(D,G) = \\mathbb{E}{x \\sim p{data}(x)} [\\log D(x)] + \\mathbb{E}_{z \\sim p(z)} [\\log (1 - D(G(z)))]\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "D* = argmax V(D,G)\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "G* = argmin V(D,G)\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "It is formulated as a minmax game, where the Discriminator is trying to maximize its reward V(D,G) and the Generator is trying to minimize the Discriminator's reward (or maximize its loss).\n",
        "The first term represents the expected value of $logD(x)$ over the real data distribution $p_{data}(x)$, while the second term represents the expected value of $log(1−D(G(z)))$ over the generated data distribution p_g(z), where z is a random noise vector sampled from a distribution $p(z)$.\n",
        "\n",
        "\n",
        "(1) Rewriting the objective function in terms of $p_{data}(x)$ and $p_z(x)$ by replacing the expectation with integrals (standard formulation for continuous random variables):<br>\n",
        "\n",
        "* For the first term we can rewrite the expectation as an integral over $p_{data}(x)$:\n",
        "\\begin{align}\n",
        "\\mathbb{E}{x \\sim p{data}(x)} [\\log D(x)] = \\int p_{data}(x) \\log D(x) dx\n",
        "\\end{align}\n",
        "\n",
        "* For the second term we can rewrite the expectation as an integral over $p_g(z)$:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbb{E}_{z \\sim p(z)} [\\log (1 - D(G(z)))] = \\int p_g(z) \\log (1 - D(G(z))) dz\n",
        "\\end{align}\n",
        "\n",
        "* To express the second term in terms of $p_g(x)$ we can perform a change of variables using the fact that $x=G(z)$. Thereby, det is a determinant of the Jacobian matrix of the inverse function $G^{-1}(x)$. By the change of the formaula:\n",
        "  \n",
        "\\begin{align}\n",
        "p_g(x) = p_g(z) \\left|\\det \\frac{\\partial G^{-1}(x)}{\\partial x}\\right|\n",
        "\\end{align}\n",
        "\n",
        "* Now substituting this expression for $p_g(z)$ into the second term we get:$\n",
        "  \n",
        "\\begin{align}\n",
        "\\int p_g(z) \\log (1 - D(G(z))) dz &= \\\\\n",
        " \\int p_g(x) \\log (1 - D(x)) \\left|\\det \\frac{\\partial G^{-1}(x)}{\\partial x}\\right| dx\n",
        "&= \\\\ \\int p_g(x) \\log (1 - D(x)) dx\n",
        "\\end{align}\n",
        "\n",
        "* Combining those terms we obtain the objective function:\n",
        "\n",
        "\\begin{align}\n",
        "V(D,G) &= \\int p_{data}(x) \\times \\log D(x) dx + \\int p_g(x) \\times \\log(1 - D(x)) dx\n",
        "\\end{align}\n",
        "\n",
        "(2) Finding the optimal discriminator $D^{*}$. Taking the derivative of the value function $V$ with respect to $D(x)$ and setting it to zero:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\delta V}{\\delta D(x)} = \\frac{p_{data}(x)}{D(x)} - \\frac{p_g(x)}{1 - D(x)} = 0\n",
        "\\end{align}\n",
        "\n",
        "Solving for D*(x):\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{p_{data}(x)}{D(x)} &= \\frac{p_g(x)}{1 - D(x)} \\\\\n",
        "p_{data}(x) (1 - D(x)) &= p_g(x) D(x) \\\\\n",
        "p_{data}(x) - p_{data}(x) D(x) &= p_g(x) D(x) \\\\\n",
        "p_{data}(x) &= p_{data}(x) D(x) + p_g(x) D(x) \\\\\n",
        "D(x) (p_{data}(x) + p_g(x)) &= p_{data}(x) \\\\\n",
        "D(x) &= \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "D_G^*(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V8ueZgnFKPF"
      },
      "source": [
        "## Task 1 b): Find Optimal Generator $G*$\n",
        "\n",
        "**Use the obtained D∗ to find the optimal point minimizing V.**\n",
        "\n",
        "Using the optimal discriminator D*:\n",
        "\\begin{align}\n",
        "D_G^*(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}\n",
        "\\end{align}\n",
        "\n",
        "(1) We can substitute this back into the original objective function $V(D,G)$:\n",
        "\n",
        "\\begin{align}\n",
        "V(D^*,G) &= \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} [\\log D^*(x)] + \\mathbb{E}_{x \\sim p_g(x)} [\\log (1 - D^*(x))] \\\\[10pt]\n",
        "\n",
        "&= \\int p_{data}(x) \\times \\log D(x) dx + \\int p_g(x) \\times \\log(1 - D(x)) dx \\\\[10pt]\n",
        "\n",
        "&= \\int p_{\\text{data}}(x) \\log \\left(\\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}\\right) dx + \\int p_g(x) \\log \\left(\\frac{p_g(x)}{p_{\\text{data}}(x) + p_g(x)}\\right) dx\n",
        "\\end{align}\n",
        "\n",
        "(2) The optimal point minimizing V occurs when $p_g(x) = p_{data}(x)$. Next, substitute this condition into the previously defined optimal discriminator function:\n",
        "\n",
        "\\begin{align}\n",
        "D_G^*(x) &= \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)} \\\\[10pt]\n",
        "&= \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_{\\text{data}}(x)} \\quad \\text{(because } p_g(x) = p_{\\text{data}}(x) \\text{)} \\\\[10pt]\n",
        "&= \\frac{p_{\\text{data}}(x)}{2p_{\\text{data}}(x)} \\\\[10pt]\n",
        "&= \\frac{1}{2}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "(3) In this case for G fixed the optimal discriminator D is:\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "D_G^*(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_{data}(x)} = \\frac{1}{2}\n",
        "\\end{align}\n",
        "\n",
        "**What value does D∗have at this point and what would this value imply?**\n",
        "<br>\n",
        "The Nash Equilibrium of the Min-Max Game is achieved when:\n",
        "* When $D*(x) = 1/2$, it means that the discriminator can't distinguish between real and generated samples. It assigns equal probability (50%) to both real and generated data.\n",
        "* This occurs when the generator has perfectly mimicked the real data distribution, i.e., $p_g(x) = p_{data}(x)$ and the discriminator cannot distinguish real from fake data.\n",
        "\n",
        "\\begin{align}\n",
        "V(D,G) = \\mathbb{E}{x \\sim p{data}(x)} [\\log D(x)] + \\mathbb{E}_{z \\sim p(z)} [\\log (1 - D(G(z)))]\n",
        "\\end{align}\n",
        "\n",
        "Substituting D*(x) = 1/2 into the objective function:<br>\n",
        "\n",
        "\\begin{align}\n",
        "V(D^*,G) &= \\log(1/2) + \\log(1/2) = -\\log(4)\n",
        "\\end{align}\n",
        "This is the global minimum of the objective function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4Z6r3JEFKPG"
      },
      "source": [
        "## Task 2: Training a GAN\n",
        "\n",
        "* Use Adam optimizer with a learning rate of 0.0001 and β1 = 0.5.\n",
        "* Fashion-MNIST has a standard resolution of 28 × 28 so make sure to resize it to 32 × 32\n",
        "* You may have to adapt the DCGAN architecture slightly to generate 32×32 images instead of 64 × 64\n",
        "* Using spectral normalization on the weights of the Discriminator can help with mode collapse and make training more stable\n",
        "* You should already see decent results after a couple of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 555,
      "metadata": {
        "id": "65HSW7EQN_-J"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "config_dict = {\n",
        "    'img_size': 32,\n",
        "    'latent_dim': 100,\n",
        "    'channels': 1,\n",
        "    'n_classes': 10,\n",
        "    'embed_dim': 16,\n",
        "    'nf': 64,\n",
        "    'batch_size': 128,\n",
        "    'num_epochs': 50,\n",
        "    'g_lr': 1e-3,\n",
        "    'd_lr': 1e-4,\n",
        "    'momentum': 0.5,\n",
        "    'betas': (0.5, 0.999),\n",
        "    'class_cond': True,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzZu5WBCN_-J"
      },
      "source": [
        "## Seminal Paper - Generative Adversarial Nets\n",
        "\n",
        "General details and background:\n",
        "* simultaneous training of two models <br>\n",
        "  (1) Discriminator, which estimates the probability that a sample came from the training data rather then from the Generator  <br>\n",
        "  (2) Generator, which captures the distribution <br>\n",
        "* represent a probability distribution over the data observed\n",
        "\n",
        "Training procesure: MinMax two-player game\n",
        "* G: maximize probability of D making a mistake\n",
        "* D: maximizing the probability to correctly estimate if a sample came from the Generator\n",
        "\n",
        "Equillibrium:\n",
        "* G recovering the true training data distribution\n",
        "* D equals to 1/2 everywhere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 556,
      "metadata": {
        "id": "1wUoXjnmFKPG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26421880/26421880 [00:23<00:00, 1112076.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 83083813.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4422102/4422102 [00:03<00:00, 1168117.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 11805509.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Download Fashion MNIST dataset\n",
        "mean = (0.5)\n",
        "std = (0.5)\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform_train = transforms.Compose([\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.RandomHorizontalFlip(),\n",
        "                                transforms.RandomRotation(5),\n",
        "                                transforms.Resize((config_dict['img_size'])),\n",
        "                                transforms.Normalize(mean, std)\n",
        "                                ])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Resize((config_dict['img_size'])),\n",
        "                                transforms.Normalize(mean, std)\n",
        "                                ])\n",
        "\n",
        "# Download and load the training data\n",
        "trainset = datasets.FashionMNIST(data_path, download=True, train=True, transform=transform_train)\n",
        "trainloader = DataLoader(trainset, batch_size=config_dict['batch_size'], shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "testset = datasets.FashionMNIST(data_path, download=True, train=False, transform=transform_test)\n",
        "testloader = DataLoader(trainset, batch_size=config_dict['batch_size'], shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 557,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "Dx3kDo3zFKPG",
        "outputId": "363023e4-3093-4c58-fd91-4c931047c172"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Get a batch of real images\n",
        "real_batch = next(iter(trainloader))\n",
        "images, _ = real_batch\n",
        "\n",
        "# Normalize the images to [0, 1] range for plotting\n",
        "images = (images + 1) / 2\n",
        "\n",
        "# Transpose the grid to match the expected shape for imshow\n",
        "grid = torchvision.utils.make_grid(images[:6], padding=2, normalize=False)\n",
        "grid = np.transpose(grid.cpu().numpy(), (1, 2, 0))\n",
        "\n",
        "# Plot the grid of images\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Training Images\")\n",
        "plt.imshow(grid)\n",
        "plt.savefig(os.path.join(result_path, 'training_images.png'))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv20m9OLN_-K"
      },
      "source": [
        "## Task 2: Training a GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 558,
      "metadata": {
        "id": "zEwm8ZDfN_-K"
      },
      "outputs": [],
      "source": [
        "# custom weights initialization called on ``netG`` and ``netD``\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "def print_parameters(net, name):\n",
        "    num_params = 0\n",
        "    for p in net.parameters():\n",
        "        num_params += p.numel()\n",
        "    \n",
        "    print(\"{} has {:.3f}M parameters\".format(name, num_params/1e6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 559,
      "metadata": {
        "id": "S_vr_wqMN_-K"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\" Generator architecture \"\"\"\n",
        "    def __init__(self, z_channels=100,  nf=64, embed_dim=32, num_classes=10, out_channels=1, class_cond=True):\n",
        "        super(Generator, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.latent_dim = z_channels\n",
        "        self.class_cond = class_cond\n",
        "        self.nf = nf\n",
        "\n",
        "        if self.class_cond:\n",
        "            self.embedding = nn.Embedding(num_classes, embed_dim) # class embeddings\n",
        "        else:\n",
        "            embed_dim = 0\n",
        "\n",
        "\n",
        "        # fully connected input layer\n",
        "        self.fc = nn.Linear(z_channels + embed_dim, 4*4*nf*8) # out: 132 x 8192\n",
        "        self.bn = nn.BatchNorm2d(4*4*nf*8)\n",
        "\n",
        "        # Convolutional layers\n",
        "        # resource: https://distill.pub/2016/deconv-checkerboard/\n",
        "        self.upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.conv1 = nn.Conv2d(nf*8, nf*4, kernel_size=3, stride=1, padding=1) # 512 x 256\n",
        "        self.bn1 = nn.BatchNorm2d(nf*4)\n",
        "        self.upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.conv2 = nn.Conv2d(nf*4, nf*2, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(nf*2)\n",
        "        self.upsample3 = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.conv3 = nn.Conv2d(nf*2, nf, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(nf)\n",
        "        self.conv4 = nn.Conv2d(nf, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # activation functions\n",
        "        self.relu = nn.ReLU(True)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, z, c=None):\n",
        "        if self.class_cond and c is not None:\n",
        "            c = self.embedding(c)\n",
        "            z = torch.cat([z, c], dim=1)\n",
        "        # Embed the input\n",
        "        x = self.fc(z)\n",
        "        x = self.relu(self.bn(x.view(x.size(0), -1, 1, 1)))  # Reshape to 4D\n",
        "\n",
        "        # Reshape the input\n",
        "        x = x.view(x.size(0), -1, 4, 4)  # Reshape to (batch_size, channels, 4, 4)\n",
        "\n",
        "        # Apply the convolutional layers\n",
        "        x = self.upsample1(x)\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.upsample2(x)\n",
        "        x = self.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.upsample3(x)\n",
        "        x = self.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.conv4(x)\n",
        "\n",
        "        # Apply a tanh activation for range [-1, 1]\n",
        "        x = self.tanh(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def set_grad(self, requires_grad=True):\n",
        "        for p in self.parameters():\n",
        "            p.requires_grad = requires_grad\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\" Discriminator architecture \"\"\"\n",
        "    def __init__(self, nf=64, embed_dim=32, out_channel=1, hidden_dim=100, dropout=0.0, num_classes=10, in_channel=1, class_cond=True):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.nf = nf\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.class_cond = class_cond\n",
        "\n",
        "        if self.class_cond:\n",
        "            self.embedding = nn.Embedding(num_classes, embed_dim)\n",
        "        else:\n",
        "            embed_dim = 0\n",
        "\n",
        "        # convolutional layers\n",
        "        self.conv1 = spectral_norm(nn.Conv2d(in_channel, nf, kernel_size=3, stride=1, padding=1, bias=True))\n",
        "        self.conv2 = spectral_norm(nn.Conv2d(nf, nf*2, kernel_size=4, stride=2, padding=1,  bias=True))\n",
        "        self.conv3 = spectral_norm(nn.Conv2d(nf*2, nf*4, kernel_size=4, stride=2, padding=1,  bias=True))\n",
        "        self.conv4 = spectral_norm(nn.Conv2d(nf*4, nf*8, kernel_size=4, stride=2, padding=1,  bias=True))\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embed = nn.Linear(4*4*nf*8 + embed_dim, hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Final output layer\n",
        "        self.fc = spectral_norm(nn.Linear(hidden_dim, out_channel, bias=True))\n",
        "\n",
        "        # activation functions\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2, inplace=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, c=None):\n",
        "        x = self.leaky_relu(self.conv1(x))\n",
        "        x = self.leaky_relu(self.conv2(x))\n",
        "        x = self.leaky_relu(self.conv3(x))\n",
        "        x = self.leaky_relu(self.conv4(x))\n",
        "\n",
        "        # Flatten the output\n",
        "        x = rearrange(x, \"b c h w -> b (c h w)\")\n",
        "\n",
        "        if self.class_cond and c is not None:\n",
        "            c = self.embedding(c)\n",
        "            x = torch.cat([x, c], dim=1)\n",
        "\n",
        "        x = self.leaky_relu(self.embed(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def set_grad(self, requires_grad=True):\n",
        "        for p in self.parameters():\n",
        "            p.requires_grad = requires_grad\n",
        "\n",
        "\n",
        "class DCGAN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 latent_dim=100,\n",
        "                 img_size=32,\n",
        "                 g_lr=1e-3,\n",
        "                 d_lr=1e-4,\n",
        "                 betas=(0.5, 0.999),\n",
        "                 momentum=1.e-5,\n",
        "                 nf=64,\n",
        "                 embed_dim=16,\n",
        "                 num_classes=10,\n",
        "                 dropout=0.0,\n",
        "                 class_cond=True,\n",
        "                 device=None):\n",
        "        super(DCGAN, self).__init__()\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "        self.img_size = img_size\n",
        "        self.d_lr = d_lr\n",
        "        self.g_lr = g_lr\n",
        "        self.betas = betas\n",
        "        self.momentum = momentum\n",
        "        self.embed_dim = embed_dim\n",
        "        self.nf = nf\n",
        "        self.num_classes = num_classes\n",
        "        self.dropout = dropout\n",
        "        self.class_cond = class_cond\n",
        "\n",
        "        if device is None:\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.input_shape = (1, self.img_size, self.img_size)\n",
        "        self.z_fixed_noise = torch.randn((1, self.latent_dim))\n",
        "        self.c_fixed = torch.LongTensor([0]).to(device)\n",
        "\n",
        "        self.real_label = 1\n",
        "        self.fake_label = 0\n",
        "\n",
        "        # Initialize Generator (G) and Discriminator (D) network\n",
        "        self.generator = Generator(z_channels=self.latent_dim, \n",
        "                                   nf=self.nf,\n",
        "                                   embed_dim=self.embed_dim,\n",
        "                                   num_classes=self.num_classes, \n",
        "                                   class_cond=self.class_cond)\n",
        "        \n",
        "        self.discriminator = Discriminator(nf=self.nf, \n",
        "                                           embed_dim=self.embed_dim, \n",
        "                                           dropout=self.dropout,\n",
        "                                           num_classes=self.num_classes, \n",
        "                                           class_cond=self.class_cond)\n",
        "\n",
        "        # Print number of parameters\n",
        "        self.print_parameters(self.generator, \"Generator\")\n",
        "        self.print_parameters(self.discriminator, \"Discriminator\")\n",
        "\n",
        "        # Initialize weights\n",
        "        self.generator.apply(self.weights_init)\n",
        "        # self.discriminator.apply(self.weights_init) - use spectral normalization instead\n",
        "\n",
        "        # Initialize loss criterion and optimizer\n",
        "        self.criterion = nn.BCEWithLogitsLoss()\n",
        "        self.optim_G = optim.Adam(self.generator.parameters(), lr=self.g_lr, betas=self.betas, weight_decay=self.momentum)\n",
        "        self.optim_D = optim.Adam(self.discriminator.parameters(), lr=self.d_lr, betas=self.betas, weight_decay=self.momentum)\n",
        "\n",
        "\n",
        "    def sample_random_z(self, img_size, latent_dim=100, device=None):\n",
        "        \"\"\" Random noise vector z.\"\"\"\n",
        "        if device is None:\n",
        "            device = self.device\n",
        "        return torch.randn((img_size, latent_dim)).to(device)\n",
        "\n",
        "    def sample_fixed_z(self, device=None):\n",
        "        \"\"\"Fixed noise vector z.\"\"\"\n",
        "        if device is None:\n",
        "            device = self.device\n",
        "        return self.z_fixed_noise.to(device), self.c_fixed.to(device)\n",
        "\n",
        "    def sample_G(self, latent_dim=100, device=None):\n",
        "        \"\"\" Generate fake images from random noise z.\"\"\"\n",
        "        if device is None:\n",
        "            device = self.device\n",
        "        z = torch.randn((latent_dim), (latent_dim)).to(device)\n",
        "        random_y = torch.LongTensor(np.expand_dims(np.linspace(0, 9, 10), 1).repeat(10,1)).to(device).flatten()\n",
        "        fake_imgs = self.generator(z, random_y)\n",
        "        return fake_imgs\n",
        "\n",
        "    def train_step(self, real_images, real_labels, device=None):\n",
        "\n",
        "        if device is None:\n",
        "            device = self.device\n",
        "\n",
        "        # if not on device move to device\n",
        "        real_images = real_images.to(device)\n",
        "        real_labels = real_labels.to(device)\n",
        "\n",
        "        # ============================================================= #\n",
        "        # ================== Train the discriminator ================== #\n",
        "        # ============================================================= #\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "\n",
        "        self.discriminator.set_grad(requires_grad=True)\n",
        "        self.optim_D.zero_grad()\n",
        "\n",
        "        # Real images\n",
        "        real_pred = self.discriminator(real_images, real_labels)\n",
        "        d_real_loss = self.criterion(real_pred, torch.ones_like(real_pred))\n",
        "\n",
        "        # Fake images\n",
        "        z_d = self.sample_random_z(img_size=real_images.size(0), latent_dim=self.latent_dim, device=self.device)\n",
        "        with torch.no_grad():\n",
        "            fake_images_d = self.generator(z_d, real_labels)\n",
        "\n",
        "        fake_pred1 = self.discriminator(fake_images_d, real_labels)\n",
        "        d_fake_loss = self.criterion(fake_pred1, torch.zeros_like(fake_pred1))\n",
        "\n",
        "        d_loss = (d_real_loss + d_fake_loss) * 0.5\n",
        "        d_loss.backward()\n",
        "        self.optim_D.step()\n",
        "\n",
        "        self.discriminator.set_grad(requires_grad=False) # freeze discriminator\n",
        "        \n",
        "        # ============================================================= #\n",
        "        # ===================== Train the generator =================== #\n",
        "        # ============================================================= #\n",
        "\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        self.optim_G.zero_grad()\n",
        "\n",
        "        # Fake images\n",
        "        z_g = self.sample_random_z(img_size=real_images.size(0), latent_dim=self.latent_dim, device=self.device)\n",
        "        fake_images_g = self.generator(z_g, real_labels)\n",
        "        fake_pred2 = self.discriminator(fake_images_g, real_labels)\n",
        "        g_loss = self.criterion(fake_pred2, torch.ones_like(fake_pred2)) # discriminator should predict 1 (real)\n",
        "\n",
        "        g_loss.backward()\n",
        "        self.optim_G.step()\n",
        "\n",
        "        # compute accuracy\n",
        "        real_acc = torch.round(torch.sigmoid(real_pred)).squeeze()\n",
        "        fake_acc = torch.round(torch.sigmoid(fake_pred1)).squeeze()\n",
        "\n",
        "        return d_real_loss, d_fake_loss, g_loss, real_acc, fake_acc\n",
        "\n",
        "    # custom weights initialization called on ``netG`` and ``netD``\n",
        "    def weights_init(self, m):\n",
        "        classname = m.__class__.__name__\n",
        "        if classname.find('Conv') != -1:\n",
        "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "        elif classname.find('BatchNorm') != -1:\n",
        "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "            nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "    def print_parameters(self, net, name):\n",
        "        num_params = 0\n",
        "        for p in net.parameters():\n",
        "            num_params += p.numel()\n",
        "\n",
        "        print(\"{} has {:.3f}M parameters\".format(name, num_params/1e6))\n",
        "    \n",
        "    def save(self, path):\n",
        "        # Save the model\n",
        "        torch.save(self.generator.state_dict(), os.path.join(path, 'generator.pth'))\n",
        "        torch.save(self.discriminator.state_dict(), os.path.join(path, 'discriminator.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 560,
      "metadata": {
        "id": "l-mE5R4GFKPG"
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, nf=64, latent_dim=100, num_classes=10) -> None:\n",
        "        super(Classifier, self).__init__()\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(1, nf, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(nf)\n",
        "        self.conv2 = nn.Conv2d(nf, nf*2, kernel_size=4, stride=2, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(nf*2)\n",
        "        self.conv3 = nn.Conv2d(nf*2, nf*4, kernel_size=4, stride=2, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(nf*4)\n",
        "        self.conv4 = nn.Conv2d(nf*4, nf*8, kernel_size=4, stride=2, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(nf*8)\n",
        "        # Dense layers\n",
        "        self.fc = nn.Linear(4*nf, latent_dim)\n",
        "        self.fc2 = nn.Linear(latent_dim, num_classes)\n",
        "\n",
        "    def get_features(self, x):\n",
        "        # Feature extraction\n",
        "        x = F.leaky_relu(self.bn1(self.conv1(x)), 0.2, inplace=True)\n",
        "        x = F.leaky_relu(self.bn2(self.conv2(x)), 0.2, inplace=True)\n",
        "        x = F.leaky_relu(self.bn3(self.conv3(x)), 0.2, inplace=True)\n",
        "        x = F.leaky_relu(self.bn4(self.conv4(x)), 0.2, inplace=True)\n",
        "        # Global average pooling\n",
        "        x = torch.mean(x, dim=(2, 3))\n",
        "        # Classification layer\n",
        "        x = F.leaky_relu(self.fc(x), 0.2, inplace=True)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass through the convolutional layers\n",
        "        x = F.leaky_relu(self.bn1(self.conv1(x)), 0.2, inplace=True)\n",
        "        x = F.leaky_relu(self.bn2(self.conv2(x)), 0.2, inplace=True)\n",
        "        x = F.leaky_relu(self.bn3(self.conv3(x)), 0.2, inplace=True)\n",
        "        x = F.leaky_relu(self.bn4(self.conv4(x)), 0.2, inplace=True)\n",
        "        x = x.view(-1, 4*self.nf)\n",
        "        x = F.leaky_relu(self.fc(x), 0.2, inplace=True)\n",
        "        # Features\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 561,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K17gOTtJN_-K",
        "outputId": "ed0e4e47-aa1d-4779-ecab-a6f1959aafb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generator has 2.525M parameters\n",
            "Discriminator has 3.575M parameters\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DCGAN(\n",
              "  (generator): Generator(\n",
              "    (embedding): Embedding(10, 16)\n",
              "    (fc): Linear(in_features=116, out_features=8192, bias=True)\n",
              "    (bn): BatchNorm2d(8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (upsample1): Upsample(scale_factor=2.0, mode='nearest')\n",
              "    (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (upsample2): Upsample(scale_factor=2.0, mode='nearest')\n",
              "    (conv2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (upsample3): Upsample(scale_factor=2.0, mode='nearest')\n",
              "    (conv3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv4): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (tanh): Tanh()\n",
              "  )\n",
              "  (discriminator): Discriminator(\n",
              "    (embedding): Embedding(10, 16)\n",
              "    (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (conv3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (conv4): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (embed): Linear(in_features=8208, out_features=100, bias=True)\n",
              "    (dropout): Dropout(p=0.0, inplace=False)\n",
              "    (fc): Linear(in_features=100, out_features=1, bias=True)\n",
              "    (leaky_relu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (sigmoid): Sigmoid()\n",
              "  )\n",
              "  (criterion): BCEWithLogitsLoss()\n",
              ")"
            ]
          },
          "execution_count": 561,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize DCGAN\n",
        "dcgan = DCGAN(    latent_dim=config_dict['latent_dim'],\n",
        "                  embed_dim=config_dict['embed_dim'],\n",
        "                  img_size=config_dict['img_size'],\n",
        "                  d_lr=config_dict['d_lr'],\n",
        "                  g_lr=config_dict['g_lr'],\n",
        "                  betas=config_dict['betas']\n",
        "                  )\n",
        "\n",
        "dcgan.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 562,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D6pl5DYyFKPH",
        "outputId": "421f3d54-40e9-44c6-a7ef-453c70e0e6f8"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[562], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Train the DCGAN\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m d_real_loss, d_fake_loss, g_loss, real_acc, fake_acc \u001b[38;5;241m=\u001b[39m \u001b[43mdcgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m cur_losses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(g_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     31\u001b[0m cur_losses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDreal\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(d_real_loss\u001b[38;5;241m.\u001b[39mitem())\n",
            "Cell \u001b[0;32mIn[559], line 238\u001b[0m, in \u001b[0;36mDCGAN.train_step\u001b[0;34m(self, real_images, real_labels, device)\u001b[0m\n\u001b[1;32m    235\u001b[0m d_fake_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(fake_pred1, torch\u001b[38;5;241m.\u001b[39mzeros_like(fake_pred1))\n\u001b[1;32m    237\u001b[0m d_loss \u001b[38;5;241m=\u001b[39m (d_real_loss \u001b[38;5;241m+\u001b[39m d_fake_loss) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m--> 238\u001b[0m \u001b[43md_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_D\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# ============================================================= #\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# ===================== Train the generator =================== #\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# ============================================================= #\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# (2) Update G network: maximize log(D(G(z)))\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/pytorch-py11/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniforge3/envs/pytorch-py11/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniforge3/envs/pytorch-py11/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "num_epochs = config_dict['num_epochs']\n",
        "\n",
        "losses_avg_train = {\"GLoss\": [], \"Dreal\": [], \"Dfake\": []}\n",
        "acc_real = []\n",
        "acc_fake = []\n",
        "\n",
        "col_fix_images = []\n",
        "\n",
        "# Train DCGAN\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "\n",
        "    dcgan.train()\n",
        "    dcgan.generator.train()\n",
        "    dcgan.discriminator.train()\n",
        "\n",
        "    cur_losses = {\"GLoss\": [], \"Dreal\": [], \"Dfake\": []}\n",
        "\n",
        "    num_total = 0\n",
        "    num_right_real = 0\n",
        "    num_right_fake = 0\n",
        "\n",
        "    for i, data in enumerate(trainloader):\n",
        "        images, targets = data\n",
        "        images = images.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Train the DCGAN\n",
        "        d_real_loss, d_fake_loss, g_loss, real_acc, fake_acc = dcgan.train_step(images, targets)\n",
        "\n",
        "        cur_losses[\"GLoss\"].append(g_loss.item())\n",
        "        cur_losses[\"Dreal\"].append(d_real_loss.item())\n",
        "        cur_losses[\"Dfake\"].append(d_fake_loss.item())\n",
        "\n",
        "        num_total += images.size(0)\n",
        "        num_right_real += torch.sum(torch.eq(real_acc, torch.ones_like(targets))).cpu().numpy()\n",
        "        num_right_fake += torch.sum(torch.eq(fake_acc, torch.zeros_like(targets))).cpu().numpy()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}'\n",
        "                  .format(epoch, num_epochs, i+1, len(trainloader), (d_real_loss.item() + d_fake_loss.item()), g_loss.item()))\n",
        "\n",
        "    # Store accuracies\n",
        "    for k,v in cur_losses.items():\n",
        "        losses_avg_train[k].append(np.mean(v))\n",
        "    acc_real.append(num_right_real/num_total*100.0)\n",
        "    acc_fake.append(num_right_fake/num_total*100.0)\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    dcgan.eval()\n",
        "    dcgan.generator.eval()\n",
        "    dcgan.discriminator.eval()\n",
        "\n",
        "    # Generate 10 x 5 images for each epoch\n",
        "    with torch.no_grad():\n",
        "        print(f\"Generating Images for Epoch: {epoch+1}\")\n",
        "        fake_imgs = dcgan.sample_G(latent_dim=100, device=device)\n",
        "        fake_imgs = F.interpolate(fake_imgs, scale_factor=2.0, mode=\"nearest\")\n",
        "        fake_imgs = (fake_imgs.cpu()+1.0)*0.5\n",
        "        grid = torchvision.utils.make_grid(fake_imgs, nrow=10)\n",
        "        grid = grid.permute(1,2,0).numpy()\n",
        "        col_fix_images.append(grid)\n",
        "        os.makedirs(\"samples\", exist_ok=True)\n",
        "        # plot images\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(f\"Fake Images Epoch {epoch+1}\")\n",
        "        plt.imshow(grid)\n",
        "        plt.show()\n",
        "        plt.imsave(f\"{result_path}/fake_images_epoch_{epoch+1}.png\", grid)\n",
        "\n",
        "    #plot losses\n",
        "    plt.figure(dpi=300)\n",
        "    plt.title(f\"GAN Losses Epoch {epoch+1}\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "\n",
        "    for k, v in losses_avg_train.items():\n",
        "            plt.plot(np.arange(1, epoch+1), v, label=k)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    plt.savefig(f\"{result_path}/losses.png\")\n",
        "\n",
        "    # plot accuracies\n",
        "    plt.figure(dpi=300)\n",
        "    plt.title(\"Classification Accuracies Discriminator\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy [%]\")\n",
        "    plt.plot(np.arange(1, epoch+1), acc_real, label=\"Real\")\n",
        "    plt.plot(np.arange(1, epoch+1), acc_fake, label=\"Fake\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    plt.savefig(f\"{result_path}/accuracy.png\")\n",
        "\n",
        "    #generate image for fixed noise vektor\n",
        "    if epoch%5==0:\n",
        "        with torch.no_grad():\n",
        "            zfix, cfix = dcgan.sample_fixed_z()\n",
        "            fix_images = dcgan.generator(zfix, cfix)\n",
        "            col_fix_images.append((F.interpolate(fix_images, scale_factor=2.0, mode=\"nearest\").cpu()+1.0)*0.5)\n",
        "\n",
        "\n",
        "# Save the model under model_path\n",
        "dcgan.save(model_path + '/dcgan.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Classifier on Fashion-MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5tY4rxsFKPH"
      },
      "outputs": [],
      "source": [
        "# Initialize the cnn model\n",
        "cnn = Classifier(nf=config_dict['img_size']).to(device)\n",
        "print_parameters(cnn, \"Classifier\")\n",
        "\n",
        "# Hyperparameters\n",
        "lr = 1e-4\n",
        "betas = (0.5, 0.999)\n",
        "\n",
        "# Train the model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(cnn.parameters(), lr=lr, betas=betas)\n",
        "\n",
        "losses_avg_train = []\n",
        "losses_avg_test = []\n",
        "acc_train = []\n",
        "acc_test = []\n",
        "\n",
        "num_epochs = 50\n",
        "epoch_decay = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    # Time measurements\n",
        "    start = time.time()\n",
        "\n",
        "    # --- Train the model --- #\n",
        "    cnn.train()\n",
        "    curr_loss = []\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, data in enumerate(trainloader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients (avoid accumulation)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = cnn(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Store the loss\n",
        "        curr_loss.append(loss.item())\n",
        "\n",
        "        # Calculate the accuracy\n",
        "        total += labels.size(0)\n",
        "        correct += torch.sum(torch.argmax(outputs, dim=1) == labels).item()\n",
        "\n",
        "    # Store the training accuracy\n",
        "    acc_train.append(correct/total*100.0)\n",
        "    losses_avg_train.append(np.mean(curr_loss))\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {np.sum(curr_loss)/len(trainloader)}\")\n",
        "\n",
        "    # --- Test the model --- #\n",
        "    cnn.eval()\n",
        "    with torch.no_grad():\n",
        "        curr_loss = []\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        for i, data in enumerate(testloader):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = cnn(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Store the loss\n",
        "            curr_loss.append(loss.item())\n",
        "\n",
        "            # Calculate the accuracy\n",
        "            total += labels.size(0)\n",
        "            correct += torch.sum(torch.argmax(outputs, dim=1) == labels).item()\n",
        "\n",
        "        # Store the test accuracy\n",
        "        acc_test.append(correct/total*100.0)\n",
        "        losses_avg_test.append(np.mean(curr_loss))\n",
        "    \n",
        "    print(f\"Test Loss: {np.mean(curr_loss)}\")\n",
        "\n",
        "\n",
        "    # Plot the training and test losses\n",
        "    plt.figure(dpi=300)\n",
        "    plt.title(\"Classification Losses\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.plot(np.arange(1, num_epochs+1), losses_avg_train, label=\"Train\")\n",
        "    plt.plot(np.arange(1, num_epochs+1), losses_avg_test, label=\"Test\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot the training and test accuracies\n",
        "    plt.figure(dpi=300)\n",
        "    plt.title(\"Classification Accuracies\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy [%]\")\n",
        "    plt.plot(np.arange(1, num_epochs+1), acc_train, label=\"Train\")\n",
        "    plt.plot(np.arange(1, num_epochs+1), acc_test, label=\"Test\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Save the model\n",
        "    if acc_test[-1] > 0.9:\n",
        "        best_acc = acc_test[-1]\n",
        "        torch.save(cnn.state_dict(), model_path + '/cnn.pth')\n",
        "    \n",
        "    # Use the learning rate scheduler\n",
        "    nlrg = lr * (1.0 - max(0, epoch - epoch_decay) / float(num_epochs - epoch_decay+1)) # linearly decay learning rate\n",
        "    if epoch > epoch_decay:\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = nlrg\n",
        "\n",
        "    \n",
        "    print(\"End of epoch {}. (Time taken: {:.3f}s)\".format(epoch, time.time() - start))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot the evaluation of Single Noise Vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#create a plot for the evaluation of a single noise vector\n",
        "grid = torchvision.utils.make_grid(torch.cat(col_fix_images, dim=0), nrow=1)\n",
        "grid = grid.permute(1,2,0).numpy()\n",
        "plt.imsave(\"gan_evolve.png\", grid)\n",
        "\n",
        "plt.figure(dpi=100)\n",
        "plt.imshow(grid)\n",
        "\n",
        "#sample per class 5 images for visualization\n",
        "samples = []\n",
        "with torch.no_grad():\n",
        "    for i in range(10):\n",
        "        z = torch.randn((5,100)).to(device)\n",
        "        target = torch.ones(5).long().to(device)*i\n",
        "        \n",
        "        imgs = dcgan.generator(z, target)\n",
        "        imgs = F.interpolate(imgs, scale_factor=2.0, mode=\"nearest\")\n",
        "        samples.append((imgs.cpu()+1.0)*0.5)\n",
        "samples = torch.cat(samples, dim=0)\n",
        "grid = torchvision.utils.make_grid(samples, nrow=5)\n",
        "grid = grid.permute(1,2,0).numpy()\n",
        "plt.imsave(\"gan_samples.png\", grid)\n",
        "\n",
        "plt.figure(dpi=200)\n",
        "plt.imshow(grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extract features from the CNN Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the model\n",
        "cnn.load_state_dict(torch.load(model_path + '/cnn.pth'))\n",
        "cnn.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWHZ35ijN_-L"
      },
      "outputs": [],
      "source": [
        "# Randomly select 5 classes from the Fashion MNIST dataset\n",
        "classes = np.random.choice(10, 5, replace=False)\n",
        "print(\"Selected classes: \", classes)\n",
        "\n",
        "n = 500\n",
        "total_samples = 5*n\n",
        "index = 0\n",
        "\n",
        "features_train_set = [] # real trainingset features\n",
        "features_test_set = [] # real testset features\n",
        "features_gan_set = [] # generated features\n",
        "\n",
        "# Track the number of examples per class\n",
        "seen_examples_per_class = np.zeros(n)\n",
        "\n",
        "# Collect features of the real training set\n",
        "while np.sum(seen_examples_per_class) < total_samples:\n",
        "    x, y = trainset[index]\n",
        "    if y in classes:\n",
        "        seen_examples_per_class[y] += 1\n",
        "        x = x.unsqueeze(0).to(device)\n",
        "        features_train_set.append(cnn.get_features(x).cpu().numpy())\n",
        "    index += 1\n",
        "features_train_set = np.concatenate(features_train_set, axis=0)\n",
        "\n",
        "# Collect features of the real test set\n",
        "index = 0\n",
        "seen_examples_per_class = np.zeros(n)\n",
        "\n",
        "while np.sum(seen_examples_per_class) < total_samples:\n",
        "    x, y = testset[index]\n",
        "    if y in classes:\n",
        "        seen_examples_per_class[y] += 1\n",
        "        x = x.unsqueeze(0).to(device)\n",
        "        features_test_set.append(cnn.get_features(x).cpu().numpy())\n",
        "    index += 1\n",
        "features_test_set = np.concatenate(features_test_set, axis=0)\n",
        "\n",
        "# Collect features of the generated images\n",
        "for i in range(5):\n",
        "    for c in 10:\n",
        "        z = torch.randn((50, 100)).to(device)\n",
        "        target = torch.ones(50).long().to(device)*i\n",
        "\n",
        "        with torch.no_grad():\n",
        "            imgs = dcgan.generator(z, target)\n",
        "            features = cnn.get_features(imgs)\n",
        "            features_gan_set.append(features.cpu().numpy())\n",
        "\n",
        "features_gan_set = np.concatenate(features_gan_set, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TSNE-Projection of Train/Test/GAN Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZCngbrSFKPI"
      },
      "outputs": [],
      "source": [
        "# Perform t-SNE on the features\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "features = np.concatenate([features_train_set, features_test_set, features_gan_set], axis=0)\n",
        "features_tsne = tsne.fit_transform(features)\n",
        "\n",
        "# Plot the t-SNE features\n",
        "plt.figure(dpi=300)\n",
        "plt.title(\"t-SNE Projection of Features\")\n",
        "plt.scatter(features_tsne[:total_samples, 0], features_tsne[:total_samples, 1], c=\"blue\", label=\"Real Training\")\n",
        "plt.scatter(features_tsne[total_samples:2*total_samples, 0], features_tsne[total_samples:2*total_samples, 1], c=\"red\", label=\"Real Test\")\n",
        "plt.scatter(features_tsne[2*total_samples:, 0], features_tsne[2*total_samples:, 1], c=\"green\", label=\"Generated\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig(\"tsne_features.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get Classification Accuracy for GAN images based on Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classification accuracy\n",
        "classes = 10\n",
        "accuracies = np.zeros(classes)\n",
        "\n",
        "for c in range(classes):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for j in range(classes):\n",
        "        # Generate 50 samples of each class\n",
        "        z = torch.randn((50, 100)).to(device)\n",
        "        target = torch.ones(50).long().to(device) * c # define target class\n",
        "\n",
        "        with torch.no_grad():\n",
        "            imgs = dcgan.generator(z, target)\n",
        "            outputs = cnn(imgs)\n",
        "            # Calculate the accuracy\n",
        "            predicted = torch.argmax(outputs, dim=1)\n",
        "            correct += torch.sum(torch.eq(predicted, target)).cpu().numpy() # count correct predictions\n",
        "\n",
        "    accuracies[c] = correct / 500 * 100.0\n",
        "\n",
        "# Output accuracies per class\n",
        "print('Accuracy per class', ', '.join(map(lambda x: \"{:.2f}%\".format(x), accuracies)))\n",
        "print('Mean accuracy {:.2f}%'.format(np.mean(accuracies)))\n",
        "\n",
        "# Accuracies per class\n",
        "plt.figure(dpi=300)\n",
        "plt.title(\"Classification Accuracy per Class\")\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Accuracy [%]\")\n",
        "plt.bar(np.arange(10), accuracies)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

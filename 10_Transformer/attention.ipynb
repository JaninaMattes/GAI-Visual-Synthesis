{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 11 Part 1: Self-Attention\n",
    "**Summer Semester 2024**\n",
    "\n",
    "**Author**: Stefan Baumann (stefan.baumann@lmu.de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Implement Self-Attention\n",
    "In this exercise, you will implement multi-head self-attention for a 2D sequence of tokens (shape `B D H W`) yourself using **only basic functions (no pre-made attention implementations!)**. You're allowed to use simple functions such as, e.g., `torch.bmm()`, `torch.nn.functional.softmax()`, ... and simple modules such as `torch.nn.Linear`.\n",
    "\n",
    "Usage of functions provided by the `einops` library (such as `einops.rearrange()`) is also allowed and encouraged (but completely optional!), as it allows writing the code in a nice and concise way by specifying operations across axes of tensors as strings instead of relying on dimension indices.<br>\n",
    "A short introduction into einops is available at https://nbviewer.org/github/arogozhnikov/einops/blob/master/docs/1-einops-basics.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device \"mps\".\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Optional\n",
    "import einops\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device \"{device}\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int = 256,\n",
    "        head_dim: int = 32,\n",
    "        value_dim: int = 32,\n",
    "        num_heads: int = 8,\n",
    "    ):\n",
    "        \"\"\"Multi-Head Self-Attention Module with 2d token input & output\n",
    "           Allows the model to jointly attend to information from different representation subspaces.\n",
    "\n",
    "        Args:\n",
    "            embed_dim (int, optional): Dimension of the tokens at the input & output (total dimensions of model). Defaults to 256.\n",
    "            head_dim (int, optional): Per-head dimension of query & key. Defaults to 32.\n",
    "            value_dim (int, optional): Per-head dimension of values (total number of features for values). Defaults to 32.\n",
    "            num_heads (int, optional): Number of parallel attention heads. Defaults to 6.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_dim = head_dim\n",
    "        self.value_dim = value_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Define linear layers for q/k/v/output \n",
    "        self.q = nn.Linear(embed_dim, num_heads * head_dim)\n",
    "        self.k = nn.Linear(embed_dim, num_heads * head_dim)\n",
    "        self.v = nn.Linear(embed_dim, num_heads * value_dim)\n",
    "        self.out = nn.Linear(num_heads * value_dim, embed_dim)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.scale = 1 / math.sqrt(self.head_dim)  # Scaling factor for attention logits 1/sqrt(head_dim)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward of multi-head self-attention\n",
    "           The convention is that each head's part in q/k/v is contiguous, \n",
    "           i.e., if you want to get the query for head 0, it's at q[..., :head_dim], head 1 is at q[..., head_dim:2*head_dim] ...\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, D, H, W) (batch, embedding dimension, height, width)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (B, D, H, W) (batch, embedding dimension, height, width)\n",
    "        \"\"\"\n",
    "        B, D, H, W = x.shape    # Batch size, Channels, Height, Width\n",
    "        N = H * W               # Number of tokens\n",
    "\n",
    "        # Reshape input to (B, N, D) for linear projections\n",
    "        x_flat = x.reshape(B, D, N).permute(0, 2, 1)  # Shape: (B, N, D)\n",
    "\n",
    "        # linear projections for q, k, v\n",
    "        Q = self.q(x_flat)\n",
    "        K = self.k(x_flat)\n",
    "        V = self.v(x_flat)\n",
    "\n",
    "        # Reshape and transpose to split heads\n",
    "        Q = Q.reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # Shape: (B, H, N, head_dim)\n",
    "        K = K.reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # Shape: (B, H, N, head_dim)\n",
    "        V = V.reshape(B, N, self.num_heads, self.value_dim).permute(0, 2, 1, 3)  # Shape: (B, H, N, value_dim)\n",
    "        \n",
    "        # Compute attention scores with scaling of attention logits by 1/sqrt(head_dim)\n",
    "        attn_scores = Q @ K.transpose(-2, -1) * self.scale\n",
    "        attn_probs = self.softmax(attn_scores)                                 \n",
    "\n",
    "        # Apply attention to values\n",
    "        attn_output = attn_probs @ V                               \n",
    "\n",
    "        # Concatenate heads and reshape to (B, N, D)\n",
    "        if self.head_dim > 1:\n",
    "            attn_output = attn_output.permute(0, 2, 1, 3).contiguous().view(B, N, -1)\n",
    "        else:\n",
    "            attn_output = attn_output.squeeze(-1)\n",
    "\n",
    "        # Apply output linear layer\n",
    "        out = self.out(attn_output)        \n",
    "\n",
    "        # Reshape back to (B, D, H, W)\n",
    "        out = out.permute(0, 2, 1).view(B, D, H, W)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in [self.q, self.k, self.v, self.out]:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "# Unit Test (single-head) DO NOT CHANGE!\n",
    "with torch.no_grad():\n",
    "    layer = SelfAttention2d(embed_dim=256, head_dim=256, value_dim=256, num_heads=1).to(device)\n",
    "    x = torch.randn((4, 256, 24, 24), device=device)\n",
    "    res_layer = layer(x)\n",
    "\n",
    "    layer_ref = nn.MultiheadAttention(layer.embed_dim, layer.num_heads).to(device)\n",
    "    layer_ref.load_state_dict({ 'in_proj_weight': torch.cat([layer.q.weight, layer.k.weight, layer.v.weight]), 'out_proj.weight': layer.out.weight }, strict=False)\n",
    "    res_ref = layer_ref(*[x.view(*x.shape[:2], -1).permute(2, 0, 1)] * 3)[0].permute(1, 2, 0).view(*x.shape)\n",
    "    assert torch.allclose(res_layer, res_ref, rtol=1e-2, atol=1e-5), 'Single-head attention result incorrect.'\n",
    "\n",
    "# Unit Test (multi-head) DO NOT CHANGE!\n",
    "with torch.no_grad():\n",
    "    layer = SelfAttention2d().to(device)\n",
    "    x = torch.randn((4, 256, 24, 24), device=device)\n",
    "    res_layer = layer(x)\n",
    "\n",
    "    layer_ref = nn.MultiheadAttention(layer.embed_dim, layer.num_heads).to(device)\n",
    "    layer_ref.load_state_dict({ 'in_proj_weight': torch.cat([layer.q.weight, layer.k.weight, layer.v.weight]), 'out_proj.weight': layer.out.weight }, strict=False)\n",
    "    res_ref = layer_ref(*[x.view(*x.shape[:2], -1).permute(2, 0, 1)] * 3)[0].permute(1, 2, 0).view(*x.shape)\n",
    "    assert torch.allclose(res_layer, res_ref, rtol=1e-2, atol=1e-5), 'Multi-head attention result incorrect.'\n",
    "\n",
    "print('All tests passed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 11 Part 2: Vision Transformers\n",
    "**Summer Semester 2024**\n",
    "\n",
    "**Author**: Stefan Baumann (stefan.baumann@lmu.de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Implement & Train a ViT\n",
    "Refer to the lecture and the original ViT paper (*AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE*, Dosovitskiy et al., 2020) for details. The naming of the hyperparameters is as in the aforementioned paper.\n",
    "\n",
    "Similar to Part 1, you're expected to implement each block yourself, although you're allowed to use blocks like `torch.nn.MultiheadAttention`, `torch.nn.Linear`, etc. Implement the blocks as in the original ViT paper. No usage of things such as full pre-made FFN/self-attention blocks or full transformer implementations like `torchvision.models.vision_transformer.VisionTransformer` is allowed for this exercise. You're expected to do full vectorized implementations in native PyTorch (again, einops is allowed) without relying on Python for loops for things such as patching etc.\n",
    "\n",
    "Some relevant details:\n",
    "- For simplicity of implementation, we will use a randomly (Gaussian with mean 0 and variance 1) initialized *learnable* positional embedding, not a Fourier/sinusoidal one.\n",
    "- Don't forget about all of the layer norms!\n",
    "- Consider the `batch_first` attribute of `nn.MultiheadAttention`, should you use that class\n",
    "- We'll make the standard assumption that $\\mathrm{dim}_\\text{head} = \\mathrm{dim}_\\text{hidden} / N_\\text{heads}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device \"mps\".\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Optional\n",
    "import einops\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device \"{device}\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualModule(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            inner_module: nn.Module\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.inner_module = inner_module\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.inner_module(x)\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    \"\"\" FeedForwardBlock class for the MLP of a transformer block in the Transformer Encoder.\n",
    "        The linear MLP layers are local and translationally equivariant, while the self-attention layers are global.\n",
    "\n",
    "        Args:\n",
    "            hidden_size (int): Hidden size of the model.\n",
    "            mlp_size (int): Size of the MLP.\n",
    "            p_dropout (float): Dropout probability.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of the feedforward block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            hidden_size: int,\n",
    "            mlp_size: int,\n",
    "            p_dropout: float\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.dropout = p_dropout\n",
    "        self.hidden_size = hidden_size # kept fixed\n",
    "        self.mlp_size = mlp_size\n",
    "\n",
    "        self.linear1 = nn.Linear(self.hidden_size, self.mlp_size)\n",
    "        self.dropout1 = nn.Dropout(self.dropout)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.linear2 = nn.Linear(self.mlp_size, self.hidden_size)\n",
    "        self.dropout2 = nn.Dropout(self.dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout2(x)\n",
    "        return x\n",
    "\n",
    "class SelfAttentionTransformerBlock(nn.Module):\n",
    "    # TODO: Student (2P)\n",
    "    # Should contain one self-attention block and use a FeedForwardBlock instance for the mlp\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            hidden_size: int,\n",
    "            n_heads: int,\n",
    "            p_dropout: float\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_heads = n_heads\n",
    "        self.p_dropout = p_dropout\n",
    "        self.mlp_size = self.hidden_size * 4 # Apparently standard in the literature\n",
    "\n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(self.hidden_size)\n",
    "        self.norm2 = nn.LayerNorm(self.hidden_size)\n",
    "\n",
    "        # Multi-head self-attention\n",
    "        self.mha = nn.MultiheadAttention(self.hidden_size, self.n_heads, dropout=p_dropout, batch_first=True)\n",
    "\n",
    "        # MLP block\n",
    "        self.mlp = FeedForwardBlock(self.hidden_size, self.mlp_size, self.p_dropout)\n",
    "\n",
    "        # Residual connection\n",
    "        self.residual1 = ResidualModule(self.norm1)\n",
    "        self.residual2 = ResidualModule(self.norm2)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(self.p_dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Residual connection\n",
    "        residual = self.residual1(x)\n",
    "\n",
    "        # Self-attention (global and permutation invariant)\n",
    "        x = self.norm1(x)\n",
    "        x = self.mha(x, x, x)[0]\n",
    "        x = self.dropout(x)\n",
    "        # Residual connection\n",
    "        x = x + residual\n",
    "\n",
    "        # Residual connection\n",
    "        residual = self.residual2(x)\n",
    "\n",
    "        # MLP (local and translationally equivariant)\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = self.dropout(x)\n",
    "        # Residual connection\n",
    "        x = x + residual\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int = 3,\n",
    "            patch_size: int = 4,\n",
    "            image_size: int = 32,\n",
    "            layers: int = 6,\n",
    "            hidden_size: int = 256,\n",
    "            mlp_size: int = 512,\n",
    "            n_heads: int = 8,\n",
    "            num_classes: int = 10,\n",
    "            p_dropout: float = 0.2,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.patch_size = patch_size\n",
    "        self.image_size = image_size\n",
    "        self.layers = layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.mlp_size = mlp_size\n",
    "        self.n_heads = n_heads\n",
    "        self.num_classes = num_classes\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "\n",
    "        # ---------------------------------\n",
    "        # ------ Transformer Encoder ------\n",
    "        # ---------------------------------\n",
    "\n",
    "        # Image patches / token\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2 # Number of patches (L) or tokens\n",
    "        self.patch_dim = self.in_channels * (self.patch_size ** 2) # Dimension of the patch after flattening (D) \n",
    "\n",
    "        # Patch embedding - linear projection of the patches\n",
    "        self.patch_embed = nn.Linear(self.patch_dim, self.hidden_size)\n",
    "\n",
    "        # Positional encoding - learnable positional embeddings\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, hidden_size))\n",
    "\n",
    "        # CLS token - classification token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.hidden_size))\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.transformer_blocks = nn.Sequential(*[\n",
    "            SelfAttentionTransformerBlock(self.hidden_size, self.n_heads, self.p_dropout)\n",
    "            for _ in range(self.layers)\n",
    "        ])\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(self.p_dropout)\n",
    "\n",
    "        # ---------------------------------\n",
    "        # ------ Classification head ------\n",
    "        # ---------------------------------\n",
    "\n",
    "        self.norm = nn.LayerNorm(self.hidden_size)\n",
    "        self.classification_head = nn.Linear(self.hidden_size, self.num_classes)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_layer_weights)\n",
    "\n",
    "    def _init_layer_weights(self, m):\n",
    "        # Initialize weights of the model\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "    def patchify(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Takes an image tensor of shape (B, C, H, W) and transforms it to a sequence of patches (B, L, D), with a learnable linear projection after flattening,\n",
    "        and a standard additive positional encoding applied. Note that the activations in (Vision) Transformer implementations are\n",
    "        typically passed around in channels-_last_ layout, different from typical PyTorch norms.\n",
    "\n",
    "        The linear projection of flattened image patches produces lower-dimensional linear embddings from flattened patches and adds positional embeddings.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, C, H, W)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Embedded patch sequence tensor with positional encodings applied and shape (B, L, D)\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # Reshape and flatten the image patches\n",
    "        x = x.reshape(B, C, H // self.patch_size, self.patch_size, W // self.patch_size, self.patch_size)\n",
    "        x = x.permute(0, 2, 4, 1, 3, 5).contiguous()             # Size: (B, H, W, C, patch_size, patch_size)\n",
    "        x = x.view(B, -1, C * self.patch_size * self.patch_size) # Size: (B, L, D) with D: C * patch_size * patch_size\n",
    "\n",
    "        # Linear projection of the patches\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # Add positional embeddings\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "\n",
    "        # Add CLS token\n",
    "        cls_token = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "\n",
    "        # Add positional embeddings for the CLS token\n",
    "        x[:, 0, :] = x[:, 0, :] + self.pos_embed[:, 0, :]\n",
    "\n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Takes an image tensor of shape (B, C, H, W), applies patching, a standard ViT \n",
    "           and then an output projection of the CLS token\n",
    "           to finally create a class logit prediction of shape (B, N_cls)\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, C, H, W)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape (B, N_cls)\n",
    "        \"\"\"\n",
    "        # Patchify input image + pos embeddings\n",
    "        x = self.patchify(x)\n",
    "\n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Transformer blocks\n",
    "        x = self.transformer_blocks(x)\n",
    "\n",
    "        # Classification head\n",
    "        x = self.norm(x[:, 0])\n",
    "        x = self.classification_head(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Do not modify this code!\n",
    "You are free to modify the four parameters in the first block, although no modifications should be necessary to achieve >70% validation accuracy with a correct transformer implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_CACHE_DIR = './data'\n",
    "BATCH_SIZE = 128\n",
    "LR = 3e-4\n",
    "N_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transforms_val = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784]),\n",
    "])\n",
    "transforms_train = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomResizedCrop((32, 32), scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784]),\n",
    "])\n",
    "\n",
    "model = VisionTransformer().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "dataloader_train = DataLoader(CIFAR10(root=DATASET_CACHE_DIR, train=True, download=True, transform=transforms_train), batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=4)\n",
    "dataloader_val = DataLoader(CIFAR10(root=DATASET_CACHE_DIR, train=False, download=True, transform=transforms_val), batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=4)\n",
    "\n",
    "train_losses = []\n",
    "val_accs = []\n",
    "\n",
    "for i_epoch in range(N_EPOCHS):\n",
    "    for i_step, (images, labels) in (pbar := tqdm(enumerate(dataloader_train), desc=f'Training (Epoch {i_epoch + 1}/{N_EPOCHS})')):\n",
    "        optim.zero_grad()\n",
    "        loss = loss_fn(model(images.to(device)), labels.to(device))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # Some logging\n",
    "        loss_val = loss.detach().item()\n",
    "        train_losses.append(loss_val)\n",
    "        pbar.set_postfix({ 'loss': loss_val } | ({ 'val_acc': val_accs[-1] } if len(val_accs) > 0 else { }))\n",
    "    \n",
    "    # Validation every epoch\n",
    "    with torch.no_grad():\n",
    "        n_total, n_correct = 0, 0\n",
    "        for i_step, (images, labels) in (pbar := tqdm(enumerate(dataloader_val), desc='Validating')):\n",
    "            predicted = model(images.to(device)).argmax(dim=-1)\n",
    "            n_correct += (predicted.cpu() == labels).float().sum().item()\n",
    "            n_total += labels.shape[0]\n",
    "        val_accs.append(n_correct / n_total)\n",
    "        print(f'Validation accuracy: {val_accs[-1]:.3f}')\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.subplot(121)\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.subplot(122)\n",
    "plt.plot(val_accs)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

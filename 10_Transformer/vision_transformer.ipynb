{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 11 Part 2: Vision Transformers\n",
    "**Summer Semester 2024**\n",
    "\n",
    "**Author**: Stefan Baumann (stefan.baumann@lmu.de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Implement & Train a ViT\n",
    "Refer to the lecture and the original ViT paper (*AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE*, Dosovitskiy et al., 2020) for details. The naming of the hyperparameters is as in the aforementioned paper.\n",
    "\n",
    "Similar to Part 1, you're expected to implement each block yourself, although you're allowed to use blocks like `torch.nn.MultiheadAttention`, `torch.nn.Linear`, etc. Implement the blocks as in the original ViT paper. No usage of things such as full pre-made FFN/self-attention blocks or full transformer implementations like `torchvision.models.vision_transformer.VisionTransformer` is allowed for this exercise. You're expected to do full vectorized implementations in native PyTorch (again, einops is allowed) without relying on Python for loops for things such as patching etc.\n",
    "\n",
    "Some relevant details:\n",
    "- For simplicity of implementation, we will use a randomly (Gaussian with mean 0 and variance 1) initialized *learnable* positional embedding, not a Fourier/sinusoidal one.\n",
    "- Don't forget about all of the layer norms!\n",
    "- Consider the `batch_first` attribute of `nn.MultiheadAttention`, should you use that class\n",
    "- We'll make the standard assumption that $\\mathrm{dim}_\\text{head} = \\mathrm{dim}_\\text{hidden} / N_\\text{heads}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Optional\n",
    "import einops\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device \"{device}\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualModule(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            inner_module: nn.Module\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.inner_module = inner_module\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.inner_module(x)\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    # TODO: Student (1P)\n",
    "    # Tip: Dropout goes after each linear layer in the feedforward block\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "class SelfAttentionTransformerBlock(nn.Module):\n",
    "    # TODO: Student (2P)\n",
    "    # Should contain one self-attention block and use a FeedForwardBlock instance for the mlp\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int = 3,\n",
    "            patch_size: int = 4,\n",
    "            image_size: int = 32,\n",
    "            layers: int = 6,\n",
    "            hidden_size: int = 256,\n",
    "            mlp_size: int = 512,\n",
    "            n_heads: int = 8,\n",
    "            num_classes: int = 10,\n",
    "            p_dropout: float = 0.2,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Student (2P)\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def patchify(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Takes an image tensor of shape (B, C, H, W) and transforms it to a sequence of patches (B, L, D), with a learnable linear projection after flattening,\n",
    "        and a standard additive positional encoding applied. Note that the activations in (Vision) Transformer implementations are\n",
    "        typically passed around in channels-_last_ layout, different from typical PyTorch norms.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, C, H, W)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Embedded patch sequence tensor with positional encodings applied and shape (B, L, D)\n",
    "        \"\"\"\n",
    "        # TODO: Student (2P)\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Takes an image tensor of shape (B, C, H, W), applies patching, a standard ViT and then an output projection of the CLS token\n",
    "        to finally create a class logit prediction of shape (B, N_cls)\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, C, H, W)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape (B, N_cls)\n",
    "        \"\"\"\n",
    "        # TODO: Student (1P)\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Do not modify this code!\n",
    "You are free to modify the four parameters in the first block, although no modifications should be necessary to achieve >70% validation accuracy with a correct transformer implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_CACHE_DIR = './data'\n",
    "BATCH_SIZE = 128\n",
    "LR = 3e-4\n",
    "N_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_val = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784]),\n",
    "])\n",
    "transforms_train = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomResizedCrop((32, 32), scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784]),\n",
    "])\n",
    "\n",
    "model = VisionTransformer().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "dataloader_train = DataLoader(CIFAR10(root=DATASET_CACHE_DIR, train=True, download=True, transform=transforms_train), batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=4)\n",
    "dataloader_val = DataLoader(CIFAR10(root=DATASET_CACHE_DIR, train=False, download=True, transform=transforms_val), batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=4)\n",
    "\n",
    "train_losses = []\n",
    "val_accs = []\n",
    "\n",
    "for i_epoch in range(N_EPOCHS):\n",
    "    for i_step, (images, labels) in (pbar := tqdm(enumerate(dataloader_train), desc=f'Training (Epoch {i_epoch + 1}/{N_EPOCHS})')):\n",
    "        optim.zero_grad()\n",
    "        loss = loss_fn(model(images.to(device)), labels.to(device))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # Some logging\n",
    "        loss_val = loss.detach().item()\n",
    "        train_losses.append(loss_val)\n",
    "        pbar.set_postfix({ 'loss': loss_val } | ({ 'val_acc': val_accs[-1] } if len(val_accs) > 0 else { }))\n",
    "    \n",
    "    # Validation every epoch\n",
    "    with torch.no_grad():\n",
    "        n_total, n_correct = 0, 0\n",
    "        for i_step, (images, labels) in (pbar := tqdm(enumerate(dataloader_val), desc='Validating')):\n",
    "            predicted = model(images.to(device)).argmax(dim=-1)\n",
    "            n_correct += (predicted.cpu() == labels).float().sum().item()\n",
    "            n_total += labels.shape[0]\n",
    "        val_accs.append(n_correct / n_total)\n",
    "        print(f'Validation accuracy: {val_accs[-1]:.3f}')\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.subplot(121)\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.subplot(122)\n",
    "plt.plot(val_accs)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
